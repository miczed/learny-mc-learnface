{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# RL Project - Learny McLearnface\n",
    "\n",
    "•\tAlbert Anguera Sempere (albert.anguerasempere@uzh.ch)   \n",
    "•\tDominik Bucher (dominik.bucher@uzh.ch)   \n",
    "•\tMichael Ziörjen (michael.zioerjen@uzh.ch)   \n",
    "\n",
    "\n",
    "\n",
    "I would like you to try to “solve” your RL problem of choice with at least two different algorithms presented in\n",
    "class:   \n",
    "• One must use a neural network as a function approximator in the context of a tabular learning algorithm or using\n",
    "policy gradient method using a neural net   \n",
    "\n",
    "You must describe your RL problem clearly in terms of the formalism of:   \n",
    "• Environment, state-space, actions, rewards   \n",
    "\n",
    "You should try (as applicable) different ways of exploration-exploitation, learning rates, starting values, etc. and\n",
    "analyze their impact\n",
    "\n",
    "Your write-up of your results should compare and contrast the algorithms and sensitivities you’ve run   \n",
    "• Graphs and tables to summarize results succinctly will help your discussion   \n",
    "\n",
    "It is required that each project submission include:   \n",
    "• A write-up of what you did and your python code   \n",
    "• Discussion as detailed below either in a jupyter notebook or a pdf file   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is a RL project structured? https://medium.com/analytics-vidhya/how-to-structure-a-reinforcement-learning-project-part-1-8a88f9025a73\n",
    "\n",
    "* Start the Journey: Frame your Problem as an RL Problem\n",
    "* Choose your Weapons: All the Tools You Need to Build a Working RL Environment\n",
    "* Face the Beast: Pick your RL (or Deep RL) Algorithm\n",
    "* Tame the Beast: Test the Performance of the Algorithm\n",
    "* Set it Free: Prepare your Project for Deployment/Publishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://gym.openai.com/envs/CarRacing-v0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CarRacing-v0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easiest continuous control task to learn from pixels, a top-down racing environment. Discreet control is reasonable in this environment as well, on/off discretisation is fine. State consists of 96x96 pixels. Reward is -0.1 every frame and +1000/N for every track tile visited, where N is the total number of tiles in track. For example, if you have finished in 732 frames, your reward is 1000 - 0.1*732 = 926.8 points. Episode finishes when all tiles are visited. Some indicators shown at the bottom of the window and the state RGB buffer. From left to right: true speed, four ABS sensors, steering wheel position, gyroscope.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}