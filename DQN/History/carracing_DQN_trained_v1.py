import gym
import numpy as np

env = gym.make('CarRacing-v0')

#######################################################################################
#
#                                     RANDOM AGENT
#
#######################################################################################

'''
for i in range(1): #Here state the number of games do you want the AI to play
    average_score=[]
    cnt=0
    done=False
    observation=env.reset()
    while not done:
        cnt+=1
        env.render()
        action = env.action_space.sample() # take a random action
        observation,reward,done,info=env.step(action)
        if done:
            print(reward)
            average_score.append(reward)
            break
    best_score=max(average_score)
env.close()
'''

#######################################################################################
#
#                                     PREPARATIONS
#
#######################################################################################

def compress_statespace(raw_statespace):

    '''

    :param raw_statespace:          The statespace as generated by gym in RGB.
                                    The statespace is an np.array with dimensions 96x96x3,
                                    which is the RGB of every pixel in the output

    :return: compressed_statespace: a compressed statespace in grayscale with
                                    dimension 7056(x1)

    this function
    - cuts away unused pixels
    - converts the state_space to grayscale
    - and normalizes the values to 0 to 1

    Function by https://github.com/elsheikh21/car-racing-ppo
    '''

    statespace_84x84 = raw_statespace[:-12, 6:-6]
    # this cuts away the lowest 12 pixels aswell as 6 left and right. returns 84x84

    compressed_statespace_84x84 = np.dot(statespace_84x84[...,0:3], [0.299, 0.587, 0.114])
    # scalar multiplication (dot-product) of every pixel with these values. these values are given by international
    # standards https://www.itu.int/rec/R-REC-BT.601-7-201103-I/en

    compressed_statespace_84x84_normalized = compressed_statespace_84x84/255.0
    # normalize the gray values to values between 0 and 1 (don't know if necessary)

    return compressed_statespace_84x84_normalized

    #compressed_statespace = compressed_statespace_84x84_normalized.flatten()
    # flat the matrix to a one-dimensional vector for the NN to read
    # don't know why elsheik is doing frame*2-1 tbh.... maybe to amplify 'color' intensity?
    #return compressed_statespace

def transform_action(action):

    '''

    :param action:                      a discretized action_space as a single integer
                                        0 = nothing
                                        1 = hard left
                                        2 = hard right
                                        3 = full accelerating
                                        4 = (mild?) breaking

    :return: quasi_continuous_action:   The action_space as generated by gym [n, n, n]
                                        for [steering, accelerating, breaking]
                                        can be -1 to 1 for steering and 0 to 1 for accelerating and breaking
                                        these are continuous values


    This function is used to transform the actions generated by the NN to a format that the environment can use

    Function by https://github.com/NotAnyMike/gym/blob/master/gym/envs/box2d/car_racing.py

    '''

    if action == 0: quasi_continuous_action = [0, 0, 0.0]  # Nothing
    elif action == 1: quasi_continuous_action = [-1, 0, 0.0]  # Left
    elif action == 2: quasi_continuous_action = [+1, 0, 0.0]  # Right
    elif action == 3: quasi_continuous_action = [0, +1, 0.0]  # Accelerate
    elif action == 4: quasi_continuous_action = [0, 0, 0.8]  # break
    else: print("action faulty for action transform", action)

    return quasi_continuous_action

def transform_action_backwards(quasi_continuous_action):

    '''
    :param: quasi_continuous_action:   The action_space as generated by gym [n, n, n]
                                        for [steering, accelerating, breaking]
                                        can be -1 to 1 for steering and 0 to 1 for accelerating and breaking
                                            hese are continuous values


    :return: action:                    a discretized action_space as a single integer
                                        0 = nothing
                                        1 = hard left
                                        2 = hard right
                                        3 = full accelerating
                                        4 = (mild?) breaking




    This function is used to transform the actions generated by the environment to a format used for indexing the NN output

    '''

    if quasi_continuous_action == [0, 0, 0.0]: action = 0  # Nothing
    elif quasi_continuous_action == [-1, 0, 0.0]: action = 1 # Left
    elif quasi_continuous_action == [+1, 0, 0.0] : action = 2  # Right
    elif quasi_continuous_action == [0, +1, 0.0]: action = 3 # Accelerate
    elif quasi_continuous_action == [0, 0, 0.8]: action = 4  # break
    else: print("action faulty for action transform", quasi_continuous_action)

    return action









# TESTING AND STUFF



observation = env.reset()                                                    # Needs to be left in for the model initiation
#for _ in range(15):
#    env.render()
#    action = env.action_space.sample()
#    observation, reward, done, info = env.step(action)
#print(observation)
#print(observation.shape)
compressed_observation = compress_statespace(observation)                   # Needs to be left in for the model initiation
#print(compressed_observation)
#print("compressed observation space shape", compressed_observation.shape)

#action = 1 # e.g. turn left
#print("before",action)
#action = transform_action(action)
#print("after",action)













#######################################################################################
#
#                                     DQN
#
#######################################################################################

import random
from keras.models import Sequential, load_model
from keras.layers import Dense, Input, Reshape
from keras.optimizers import Adam
from collections import deque
from keras.utils.vis_utils import plot_model


dqn_agent_trained = load_model("20201126p/DQNmodel_1")


done=False
score = 0

env = gym.wrappers.Monitor(env, "AppliedAgent", force=True)
cur_state = compress_statespace(env.reset())



for i in range(300):

    env.render()

    state = np.reshape(cur_state,(1,84,84))
    action = np.argmax(dqn_agent_trained.predict(state))

    print(dqn_agent_trained.predict(state))
    #print(action)

    action = transform_action(action)

    new_state, reward, done, _ = env.step(action)
    new_state = compress_statespace(new_state)

    score = reward + score
    cur_state = new_state

    if done:
        print("DONE!")
        break

print(score)
env.close()

