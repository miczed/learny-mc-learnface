import gym
import numpy as np

env = gym.make('CarRacing-v0')

#######################################################################################
#
#                                     RANDOM AGENT
#
#######################################################################################

'''
for i in range(1): #Here state the number of games do you want the AI to play
    average_score=[]
    cnt=0
    done=False
    observation=env.reset()
    while not done:
        cnt+=1
        env.render()
        action = env.action_space.sample() # take a random action
        observation,reward,done,info=env.step(action)
        if done:
            print(reward)
            average_score.append(reward)
            break
    best_score=max(average_score)
env.close()
'''

#######################################################################################
#
#                                     PREPARATIONS
#
#######################################################################################

def compress_statespace(raw_statespace):

    '''

    :param raw_statespace:          The statespace as generated by gym in RGB.
                                    The statespace is an np.array with dimensions 96x96x3,
                                    which is the RGB of every pixel in the output

    :return: compressed_statespace: a compressed statespace in grayscale with
                                    dimension 7056(x1)

    this function
    - cuts away unused pixels
    - converts the state_space to grayscale
    - and normalizes the values to 0 to 1

    Function by https://github.com/elsheikh21/car-racing-ppo
    '''

    statespace_84x84 = raw_statespace[:-12, 6:-6]
    # this cuts away the lowest 12 pixels aswell as 6 left and right. returns 84x84

    compressed_statespace_84x84 = np.dot(statespace_84x84[...,0:3], [0.299, 0.587, 0.114])
    # scalar multiplication (dot-product) of every pixel with these values. these values are given by international
    # standards https://www.itu.int/rec/R-REC-BT.601-7-201103-I/en

    compressed_statespace_84x84_normalized = compressed_statespace_84x84/255.0
    # normalize the gray values to values between 0 and 1 (don't know if necessary)

    return compressed_statespace_84x84_normalized

    #compressed_statespace = compressed_statespace_84x84_normalized.flatten()
    # flat the matrix to a one-dimensional vector for the NN to read
    # don't know why elsheik is doing frame*2-1 tbh.... maybe to amplify 'color' intensity?
    #return compressed_statespace

def transform_action(action):

    '''

    :param action:                      a discretized action_space as a single integer
                                        0 = nothing
                                        1 = hard left
                                        2 = hard right
                                        3 = full accelerating
                                        4 = (mild?) breaking

    :return: quasi_continuous_action:   The action_space as generated by gym [n, n, n]
                                        for [steering, accelerating, breaking]
                                        can be -1 to 1 for steering and 0 to 1 for accelerating and breaking
                                        these are continuous values


    This function is used to transform the actions generated by the NN to a format that the environment can use

    Function by https://github.com/NotAnyMike/gym/blob/master/gym/envs/box2d/car_racing.py

    '''

    if action == 0: quasi_continuous_action = [0, 0, 0.0]  # Nothing
    elif action == 1: quasi_continuous_action = [-1, 0, 0.0]  # Left
    elif action == 2: quasi_continuous_action = [+1, 0, 0.0]  # Right
    elif action == 3: quasi_continuous_action = [0, 0.5, 0.0]  # Accelerate
    elif action == 4: quasi_continuous_action = [0, 0, +1]  # break
    else: print("action faulty for action transform", action)

    return quasi_continuous_action

def transform_action_backwards(quasi_continuous_action):

    '''
    :param: quasi_continuous_action:   The action_space as generated by gym [n, n, n]
                                        for [steering, accelerating, breaking]
                                        can be -1 to 1 for steering and 0 to 1 for accelerating and breaking
                                            hese are continuous values


    :return: action:                    a discretized action_space as a single integer
                                        0 = nothing
                                        1 = hard left
                                        2 = hard right
                                        3 = full accelerating
                                        4 = (mild?) breaking




    This function is used to transform the actions generated by the environment to a format used for indexing the NN output

    '''

    if quasi_continuous_action == [0, 0, 0.0]: action = 0  # Nothing
    elif quasi_continuous_action == [-1, 0, 0.0]: action = 1 # Left
    elif quasi_continuous_action == [+1, 0, 0.0] : action = 2  # Right
    elif quasi_continuous_action == [0, 0.5, 0.0]: action = 3 # Accelerate
    elif quasi_continuous_action == [0, 0, +1]: action = 4  # break
    else: print("action faulty for action transform", quasi_continuous_action)

    return action









# TESTING AND STUFF



observation = env.reset()                                                    # Needs to be left in for the model initiation
#for _ in range(15):
#    env.render()
#    action = env.action_space.sample()
#    observation, reward, done, info = env.step(action)
#print(observation)
#print(observation.shape)
compressed_observation = compress_statespace(observation)                   # Needs to be left in for the model initiation
#print(compressed_observation)
#print("compressed observation space shape", compressed_observation.shape)

#action = 1 # e.g. turn left
#print("before",action)
#action = transform_action(action)
#print("after",action)













#######################################################################################
#
#                                     DQN
#
#######################################################################################

import random
from keras.models import Sequential
from keras.layers import Dense, Input, Reshape
from keras.optimizers import Adam
from keras import losses
from tensorflow import GradientTape
from collections import deque
from keras.utils.vis_utils import plot_model

class DQN:
    def __init__(self, env):
        self.env = env

        self.gamma = 0.8           # discount factor
        self.epsilon = 1            # exploration vs explotation, i.e. rate to deviate to random actions
        self.epsilon_min = 0.1     # minimum epsilon
        self.epsilon_decay = 0.99  # how epsilon evolves (we want much exploration at the beginning and only few in the end)
        self.learning_rate = 0.25  # ???
        self.tau = 0.8             # rate to update target (goal) model
        #self.tau_decay = 0.999           # own idea: maybe learn faster at the beginning?
        #self.tau_min = 0.1

        self.model = self.create_model()            # create model (what actions to take)
        self.target_model = self.create_model()     # and target model (and what actions we want it to take) this is done not to vary the goal while training. i.e. make the model more stable
                                                    # MachineLearningWithPhil calls the two models: next and eval: for

        self.mem_cntr = 0
        self.mem_size = 100000
        self.batch_size = 5
        self.state_memory = np.zeros((self.mem_size, *compressed_observation.shape), dtype=np.float32)
        self.new_state_memory = np.zeros((self.mem_size, *compressed_observation.shape), dtype=np.float32)
        self.action_memory = np.zeros((self.mem_size), dtype=int)
        self.reward_memory = np.zeros((self.mem_size), dtype=np.float32)
        self.terminal_memory = np.zeros((self.mem_size), dtype=np.bool)



    def create_model(self):         # initiate model and add layers to NN, define activation function and shapes of in- and output
        model = Sequential()

        model.add(Input(shape=(compressed_observation.shape)))
        model.add(Reshape((7056,)))                                 # replaces the flatten from the compress statespace
        model.add(Dense(100, name = "dense_layer", activation="relu"))
        model.add(Dense(5, name = "action_layer"))

        #print("Input shape", model.input_shape)
        #print("Output shape", model.output_shape)
        model.compile(optimizer=Adam(lr=self.learning_rate))
        #print(model.summary())
        return model


    def act(self, state):                                   # funtion to actually perform actions. input state, returns action

        # NOT COMPRESS AS THE FUNCTION IS ONLY FED WITH COMPRESSED

        self.epsilon *= self.epsilon_decay                  # let epsilon decay
        self.epsilon = max(self.epsilon_min, self.epsilon)  # make sure it's not lower than minimum
        if np.random.random() < self.epsilon:               # randomly decide if exploit or explore
            #print("DEBUG explore")
            random_action = random.randint(0,4)
            #print(random_action)
            return random_action                               # explore

        else:
            #print("DEBUG exploit")
            state = np.reshape(state,(1,84,84)) # this function somehow fixes the act() and replay() part, however i have no idea if it's still doing what it should or why this fixes it
            # exploit : the model predicts for each of the five actions the resulting Q value .
            # we choose the "action" (with argmax) highest Q value
            # should return integer e.g. 2 for "left", will be transformed in main()
            #print(self.model.predict(state))
            return np.argmax(self.model.predict(state))


    def remember(self, state, action, reward, new_state, done):         # remember previous state, action, reward

        '''

        https://www.youtube.com/watch?v=wc-FxNENg9U
        this function was rewritten following the Machine Learning with Phil Tutorial

        '''

        index = self.mem_cntr % self.mem_size
        self.state_memory[index] = state
        self.action_memory[index] = action
        self.reward_memory[index] = reward
        self.new_state_memory[index] = new_state
        self.terminal_memory[index] = done

        self.mem_cntr += 1










    def replay(self):

        '''

        https://www.youtube.com/watch?v=wc-FxNENg9U
        this function was rewritten following the Machine Learning with Phil Tutorial

        '''

        if self.mem_cntr < self.batch_size:
            return

        max_mem = min(self.mem_cntr, self.mem_size)
        batch = np.random.choice(max_mem, self.batch_size, replace=False)
        batch_index = np.arange(self.batch_size, dtype=np.int32)

        states = self.state_memory[batch]
        actions = self.action_memory[batch]
        rewards = self.reward_memory[batch]
        new_states = self.new_state_memory[batch]
        terminal = self.terminal_memory[batch]
        indices = np.arange(self.batch_size)

        q_pred = self.model.predict(states)[batch_index, actions]
        q_next = self.target_model.predict(states)
        q_next[terminal] = 0.0

        q_target = rewards + self.gamma * np.amax(q_next, 1)

        #print(q_pred)
        #print(q_target)

        customMSE = losses.mean_squared_error(y_pred=q_pred, y_true=q_target)



        #grads = GradientTape.gradient(customMSE, [self.model.trainable_weights])
        #optimizer.apply_gradients(zip(grads, [self.model.trainable_weights]))

        # TODO PHIL USES PREDICT VS TRUE LOSS
        # TODO MOUNTAINCAR USES STATES TO TRUE OPTIMIZER
        # TODO NEED TO CHANGE TO LOSS https://keras.io/guides/writing_a_training_loop_from_scratch/
        # TODO probably the most inefficient solution right now with the minimize MSE




    def target_train(self):                                     # reorient goals, i.e. copy the weights from the main model into the target model

        #self.tau *= self.tau_decay  # let tau decay
        #self.tau = max(self.tau_min, self.tau)  # make sure it's not lower than minimum

        weights = self.model.get_weights()                      # since this is done less frequently, it doesn't distort goals while training
        target_weights = self.target_model.get_weights()
        for i in range(len(target_weights)):
            target_weights[i] = weights[i] * self.tau + target_weights[i] * (1 - self.tau) # adjust target weights at rate tau
        self.target_model.set_weights(target_weights)

    def save_model(self, fn):
        self.target_model.save(fn)








import time

start = time.time()

TRIAL_ID = "20201126q"


def main():
    env = gym.make("CarRacing-v0")
    env = gym.wrappers.Monitor(env, "{}/recordings".format(TRIAL_ID), force=True)

    trials = 10          # aka episodes (original 1000)
    trial_len = 200     # how long one episode is (don't know what makes sense? must be over 900 right?)

    dqn_agent = DQN(env=env)
    step = []
    for trial in range(trials):

        cur_state = compress_statespace(env.reset())         # COMPRESS current state
        score = 0
        tiles = 0

        for step in range(trial_len):

            if step % 100 == 0: #print every n-th step
                print("trial:", trial, "of", trials-1, "| step:",step, "of",trial_len-100)

            num_action = dqn_agent.act(cur_state)               # act given current state, either explore or exploit
            #print("\tact: ", action)
            #print("DEBUG main: action by dqn:", action)
            action = transform_action(num_action)               # TRANSFORM ACTION
            #print("DEBUG main: action to step:", action)

            new_state, reward, done, _ = env.step(action)   # actual result of act chosen by dqn_agent.act()
            new_state = compress_statespace(new_state)      # COMPRESS new state

            score = score + reward
            if reward >=0:
                tiles += 1

            #print("\tremember")
            dqn_agent.remember(cur_state, num_action, reward, new_state, done)
            #print("\treplay")
                                        # internally iterates default (prediction) model
            dqn_agent.replay()
            #print("\ttrain")
            dqn_agent.target_train()                        # iterates target model

            cur_state = new_state

            if done:
                print("done in trial:",trial,". With score:",score)
                env.stats_recorder.save_complete()
                break

        if score <= 100:                                                 # after 'for loop' finishes or done, check if score is <900 then print fail         # TODO score >900
            print("Finished trial {}, but only reached {} points ({} tiles)".format(trial, round(score,0), tiles))
            dqn_agent.save_model("{}/DQNmodel_{}".format(TRIAL_ID, trial))
            env.stats_recorder.save_complete()
            env.stats_recorder.done = True

        else:                                                           # after 'for loop' finishes or done, check if step >900 then print success
            print("COMPLETED!!! reached {} points at step {} in trial {}".format(score, step, trial))
            dqn_agent.save_model("{}/DQNmodel_SUCCESSFUL".format(TRIAL_ID))
            env.stats_recorder.save_complete()
            env.stats_recorder.done = True
            break

    plot_model(dqn_agent.model, to_file='{}/DQNmodelNN.png'.format(TRIAL_ID), show_shapes=True, show_layer_names=True)


if __name__ == "__main__":
    main()


end = time.time()
print("Elapsed time:", round((end-start)/60,1)," Minutes")

import os
os.system('play -nq -t alsa synth {} sine {}'.format(0.2, 400))
os.system('play -nq -t alsa synth {} sine {}'.format(0.1, 600))

