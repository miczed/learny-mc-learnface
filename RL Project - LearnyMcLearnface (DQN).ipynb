{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Report_Resources/1200px-Universität_Zürich_logo.svg.png\" style=\"width:400px;height:180px;\">\n",
    "\n",
    "# RL Project - Learny McLearnface\n",
    "\n",
    "* Albert Anguera Sempere (albert.anguerasempere@uzh.ch)\n",
    "* Dominik Bucher (dominik.bucher@uzh.ch)\n",
    "* Michael Ziörjen (michael.zioerjen@uzh.ch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Deep Q Network Approach\n",
    "\n",
    "Following the preliminary code of the DQN solution approach:\n",
    "\n",
    "### Preparations\n",
    "\n",
    "This section contains functions for dealing with discretization and plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "def compress_statespace(raw_statespace):\n",
    "    '''\n",
    "    :param raw_statespace:          The statespace as generated by gym in RGB.\n",
    "                                    The statespace is an np.array with dimensions 96x96x3,\n",
    "                                    which is the RGB of every pixel in the output\n",
    "    :return: compressed_statespace: a compressed statespace in grayscale with\n",
    "                                    dimension 7056(x1)\n",
    "    this function\n",
    "    - cuts away unused pixels\n",
    "    - converts the state_space to grayscale\n",
    "    - and normalizes the values to 0 to 1\n",
    "    Function by https://github.com/elsheikh21/car-racing-ppo\n",
    "    '''\n",
    "\n",
    "    statespace_84x84 = raw_statespace[:-12, 6:-6]\n",
    "    # this cuts away the lowest 12 pixels aswell as 6 left and right. returns 84x84\n",
    "    compressed_statespace_84x84 = np.dot(statespace_84x84[...,0:3], [0.299, 0.587, 0.114])\n",
    "    # scalar multiplication (dot-product) of every pixel with these values. these values are given by\n",
    "    # international standards https://www.itu.int/rec/R-REC-BT.601-7-201103-I/en\n",
    "    compressed_statespace_84x84_normalized = compressed_statespace_84x84/255.0\n",
    "    # normalize the gray values to values between 0 and 1 (don't know if necessary)\n",
    "    #return compressed_statespace_84x84_normalized\n",
    "    compressed_statespace = compressed_statespace_84x84_normalized.flatten()\n",
    "    # flat the matrix to a one-dimensional vector for the NN to read\n",
    "    # don't know why elsheik is doing frame*2-1 tbh.... maybe to amplify 'color' intensity?\n",
    "    return compressed_statespace\n",
    "\n",
    "def compress_statespace_light(raw_statespace):\n",
    "    '''\n",
    "\n",
    "    :param raw_statespace:          The statespace as generated by gym in RGB.\n",
    "                                    The statespace is an np.array with dimensions 96x96x3,\n",
    "                                    which is the RGB of every pixel in the output\n",
    "    :return: compressed_statespace: a compressed statespace in grayscale with\n",
    "                                    dimension 84x84(x1)\n",
    "\n",
    "    this function\n",
    "    - cuts away unused pixels\n",
    "    - converts the state_space to grayscale\n",
    "    - and normalizes the values to 0 to 1\n",
    "    - NOTE: LIGHT because it does not flatten -> used for Conv2D\n",
    "    Function by https://github.com/elsheikh21/car-racing-ppo\n",
    "    '''\n",
    "\n",
    "    statespace_84x84 = raw_statespace[:-12, 6:-6]\n",
    "    # this cuts away the lowest 12 pixels aswell as 6 left and right. returns 84x84\n",
    "    compressed_statespace_84x84 = np.dot(statespace_84x84[...,0:3], [0.299, 0.587, 0.114])\n",
    "    # scalar multiplication (dot-product) of every pixel with these values. these values are given by international\n",
    "    # standards https://www.itu.int/rec/R-REC-BT.601-7-201103-I/en\n",
    "    compressed_statespace_84x84_normalized = compressed_statespace_84x84/255.0\n",
    "    # normalize the gray values to values between 0 and 1 (don't know if necessary)\n",
    "    return compressed_statespace_84x84_normalized\n",
    "\n",
    "def transform_action(action):\n",
    "    '''\n",
    "    :param action:                      a discretized action_space as a single integer\n",
    "                                        0 = nothing\n",
    "                                        1 = hard left\n",
    "                                        2 = hard right\n",
    "                                        3 = full accelerating\n",
    "                                        4 = (mild?) breaking\n",
    "    :return: quasi_continuous_action:   The action_space as generated by gym [n, n, n]\n",
    "                                        for [steering, accelerating, breaking]\n",
    "                                        can be -1 to 1 for steering and 0 to 1 for accelerating and breaking\n",
    "                                        these are continuous values\n",
    "\n",
    "    This function is used to transform the actions generated by the NN to a format that the environment can use\n",
    "    Function by https://github.com/NotAnyMike/gym/blob/master/gym/envs/box2d/car_racing.py\n",
    "    '''\n",
    "    if action == 0: quasi_continuous_action = [0, 0, 0.0]  # Nothing\n",
    "    elif action == 1: quasi_continuous_action = [-1, 0, 0.0]  # Left\n",
    "    elif action == 2: quasi_continuous_action = [+1, 0, 0.0]  # Right\n",
    "    elif action == 3: quasi_continuous_action = [0, +1, 0.0]  # Accelerate\n",
    "    elif action == 4: quasi_continuous_action = [0, 0, +1]  # break\n",
    "    else: print(\"action faulty for action transform\", action)\n",
    "\n",
    "    return quasi_continuous_action\n",
    "\n",
    "def plot_learning_curve(x, scores, epsilons, filename, reload=None):\n",
    "    '''\n",
    "    Args:\n",
    "        x: counter of episodes\n",
    "        scores: socre per episode\n",
    "        epsilons: epsilon per episode\n",
    "        filename: where to store plot\n",
    "        reload: used to continue plotting a resumed training\n",
    "    Returns: a plot\n",
    "    Function by 'ML with Phil' https://github.com/philtabor/Deep-Q-Learning-Paper-To-Code/blob/master/utils.py\n",
    "    '''\n",
    "\n",
    "    if reload is not None:\n",
    "        plot_data = csv.reader(\"plot_data.csv\", delimiter=\";\")\n",
    "        # TODO finish function to plot graphs when resuming training\n",
    "\n",
    "    fig=plt.figure()\n",
    "    ax=fig.add_subplot(111, label=\"1\")\n",
    "    ax2=fig.add_subplot(111, label=\"2\", frame_on=False)\n",
    "\n",
    "    ax.plot(x, epsilons, color=\"C0\")\n",
    "    ax.set_xlabel(\"Episode\", color=\"C0\")\n",
    "    ax.set_ylabel(\"Epsilon\", color=\"C0\")\n",
    "    ax.tick_params(axis='x', colors=\"C0\")\n",
    "    ax.tick_params(axis='y', colors=\"C0\")\n",
    "\n",
    "    N = len(scores)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "\t    running_avg[t] = np.mean(scores[max(0, t-20):(t+1)])\n",
    "\n",
    "    ax2.scatter(x, scores, color=\"C1\")\n",
    "    ax2.plot(x, running_avg, color=\"C1\")\n",
    "    ax2.axes.get_xaxis().set_visible(False)\n",
    "    ax2.yaxis.tick_right()\n",
    "    ax2.set_ylabel('Score and MA20', color=\"C1\")\n",
    "    ax2.yaxis.set_label_position('right')\n",
    "    ax2.tick_params(axis='y', colors=\"C1\")\n",
    "\n",
    "    plt.savefig(filename)\n",
    "    plt.close('all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Agent\n",
    "\n",
    "This section contains the code for the model and as well as the agent.\n",
    "I also tried an implementation with Conv2d layers which only differs minimally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "class DeepQNetwork:\n",
    "    '''\n",
    "    LECTURE 4   page 4: Neural Networks in general and why they are good for RL\n",
    "                page 26: how many layers\n",
    "                page 34: issues with NN\n",
    "                page 35: features should be normalized (done in function compress() )\n",
    "                page 39; learning rate\n",
    "                page 53: overfitting (obviously not a problem rn)\n",
    "                page 59: early stop (not yet)\n",
    "\n",
    "                https://keras.io/api/layers/convolution_layers/convolution2d/\n",
    "                https://towardsdatascience.com/reinforcement-learning-w-keras-openai-dqns-1eed3a5338c\n",
    "                https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf\n",
    "    '''\n",
    "\n",
    "    def __init__(self, gamma, epsilon, lr, epsilon_min, epsilon_decay, tau, batch_size, mem_size,\n",
    "                 reload=False, reload_path=None):\n",
    "\n",
    "        self.lr = lr\n",
    "        self.reload = reload\n",
    "        self.reload_path = reload_path\n",
    "        if self.reload == True:\n",
    "            self.model = self.load_model(self.reload_path)\n",
    "            self.target_model = self.load_model(self.reload_path)\n",
    "        else:\n",
    "            self.model = self.create_model()\n",
    "            self.target_model = self.create_model()\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.tau = tau\n",
    "\n",
    "        self.mem_cntr = 0\n",
    "        self.mem_size = mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=self.mem_size)\n",
    "\n",
    "    def load_model(self, path):\n",
    "\n",
    "        model = load_model(path)\n",
    "        return model\n",
    "\n",
    "    def create_model(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        input_shape = (7056,)\n",
    "        model.add(Input(shape=input_shape))\n",
    "        model.add(Dense(units=100, activation=\"relu\"))\n",
    "        model.add(Dense(units=5))\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(lr=self.lr))\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        '''\n",
    "        LECTURE: 2 page 41: Explore or exploit\n",
    "        '''\n",
    "\n",
    "        # funtion to actually perform actions. input state, returns action\n",
    "        # NOT COMPRESS AS THE FUNCTION IS ONLY FED WITH COMPRESSED\n",
    "        self.epsilon *= self.epsilon_decay                  # let epsilon decay\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon)  # make sure it's not lower than minimum\n",
    "\n",
    "        if np.random.random() < self.epsilon:               # randomly decide if exploit or explore\n",
    "            #print(\"DEBUG explore\")\n",
    "            random_action = random.randint(0,4)\n",
    "            #print(random_action)\n",
    "            return random_action                               # explore\n",
    "\n",
    "        else:\n",
    "            #print(\"DEBUG exploit\")\n",
    "            # exploit : the model predicts for each of the five actions the resulting Q value .\n",
    "            # we choose the \"action\" (with argmax) highest Q value\n",
    "            # should return integer e.g. 2 for \"left\", will be transformed in main()\n",
    "            state = np.reshape(state,(1,7056))    # solves channel and batch issue i guess TODO verify\n",
    "            # nolonger used after change to torch\n",
    "            pred = self.model.predict(state)\n",
    "            #print(pred)\n",
    "            action = np.argmax(pred)\n",
    "            #print(action)\n",
    "            return action\n",
    "\n",
    "    def remember(self, state, num_action, reward, new_state, done):\n",
    "        # remember previous state, action, reward\n",
    "        self.memory.append([state, num_action, reward, new_state, done])\n",
    "\n",
    "    def replay(self):\n",
    "        '''\n",
    "        LECUTRE:    2 page 39 batch vs oneline\n",
    "                    2 page 40 MeanSquearedError for Q approximation\n",
    "                    2 page 42 Cross-entropy error?\n",
    "                    2 page 44 experience replay\n",
    "        '''\n",
    "\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            #print(\"not yet\")\n",
    "            return\n",
    "\n",
    "        samples = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        for sample in samples:\n",
    "            state, action_num, reward, new_state, done = sample  # get a random state from the samples\n",
    "            state = np.reshape(state, (1, 7056))  # solves channel and batch issue i guess TODO verify\n",
    "            new_state = np.reshape(new_state, (1, 7056))  # solves channel and batch issue i guess TODO verify\n",
    "            Q_pred = self.model.predict(state)  # predict what to do with base model, given random state\n",
    "            Q_true = Q_pred # declare to \"true\" Q value\n",
    "            if done:\n",
    "                Q_pred[0][action_num] = reward  # does it return done? if yes nice, put in final reward\n",
    "            else:\n",
    "                Q_future = max(self.target_model.predict(new_state)[0])  # what is the future value of that state after the action that was taken (given one takes the best action next)\n",
    "                Q_true[0][action_num] = reward + Q_future * self.gamma - Q_pred[0][action_num] # adjust the \"true\" Q value with immediate reward, and discounted future Q-value for tha action that was taken\n",
    "                #print(\"Debug replay: Q_true:\", Q_true)\n",
    "\n",
    "            self.model.fit(state, Q_true, epochs=1, verbose=0)\n",
    "\n",
    "    def target_train(self):\n",
    "        # reorient goals, i.e. copy the weights from the main model into the target model\n",
    "\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = weights[i] * self.tau + target_weights[i] * (1 - self.tau)\n",
    "        self.target_model.set_weights(target_weights)\n",
    "\n",
    "\n",
    "    def save_model(self, name):\n",
    "\n",
    "        self.target_model.save(name)\n",
    "\n",
    "    def print_summary(self):\n",
    "\n",
    "        print(self.model.summary())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Main\n",
    "\n",
    "The function that actually runs the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/gym/logger.py:30: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 100)               705700    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 706,205\n",
      "Trainable params: 706,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Track generation: 1117..1404 -> 287-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1056..1324 -> 268-tiles track\n",
      "\tTrial: 0 of 21 | Step: 0 of 1100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    env = gym.make(\"CarRacing-v0\")\n",
    "    env = gym.wrappers.Monitor(env, \"DQN/Models/{}/recordings\".format(TRIAL_ID), force=True, video_callable=lambda episode_id:True)\n",
    "\n",
    "    agent = DeepQNetwork(tau=0.25,\n",
    "                         lr=0.01,                   # 0.01 by Aldape and Sowell\n",
    "                         gamma=0.99,\n",
    "                         epsilon=1,\n",
    "                         epsilon_decay=0.9995,\n",
    "                         epsilon_min=0.1,          # 0.1 by Aldape amd Sowell\n",
    "                         batch_size=32,\n",
    "                         mem_size= 10000,\n",
    "                         reload=False,\n",
    "                         reload_path=\"DQN/Models/20201129/DQNmodel\")\n",
    "\n",
    "    trials = 22         # aka episodes (Aldape and Sowell >>1000)\n",
    "    trial_len = 1200     # how long one episode is. must be greater, but not needed yet\n",
    "\n",
    "    step = []\n",
    "    score_hist = []\n",
    "    eps_hist = []\n",
    "    trial_array = []\n",
    "\n",
    "    agent.print_summary()\n",
    "\n",
    "    for trial in range(trials):\n",
    "\n",
    "        start_trial = time.time()\n",
    "\n",
    "        cur_state = compress_statespace(env.reset())         # COMPRESS current state\n",
    "        score = 0\n",
    "        tiles = 0\n",
    "\n",
    "        for step in range(trial_len):\n",
    "\n",
    "            if step % 100 == 0: #print every n-th step\n",
    "                print(\"\\tTrial:\", trial, \"of\", trials-1, \"| Step:\",step, \"of\",trial_len-100)\n",
    "\n",
    "            num_action = agent.act(cur_state)               # act given current state, either explore or exploit\n",
    "\n",
    "            #print(\"\\tact: \", num_action)\n",
    "            #print(\"DEBUG main: action by dqn:\", num_action)\n",
    "            action = transform_action(num_action)               # TRANSFORM ACTION\n",
    "            #print(\"DEBUG main: action to step:\", action)\n",
    "            new_state, reward, done, _ = env.step(action)   # actual result of act chosen by dqn_agent.act()\n",
    "            new_state = compress_statespace(new_state)      # COMPRESS new state\n",
    "            score += reward\n",
    "            if reward >= 0:\n",
    "                tiles += 1\n",
    "            #print(\"\\tremember\")\n",
    "            agent.remember(cur_state, num_action, reward, new_state, done)\n",
    "            #print(\"\\treplay\")\n",
    "            # internally iterates default (prediction) model\n",
    "            agent.replay()\n",
    "            #print(\"\\ttrain\")\n",
    "            agent.target_train()\n",
    "            cur_state = new_state\n",
    "\n",
    "            if done:\n",
    "                env.stats_recorder.save_complete()\n",
    "                break\n",
    "\n",
    "        score_hist.append(score)\n",
    "        trial_array.append(trial)\n",
    "        eps_hist.append(agent.epsilon)\n",
    "        plot_learning_curve(x=trial_array, scores=score_hist, epsilons=eps_hist, filename=\"Models/{}/{}\".format(TRIAL_ID, \"Performance\"))\n",
    "        plot_data = np.array([trial_array, score_hist, eps_hist])\n",
    "        np.savetxt(\"DQN/Models/{}/plot_data.csv\".format(TRIAL_ID), plot_data, delimiter=\";\")\n",
    "\n",
    "        end_trial = time.time()\n",
    "        time_trial = round((end_trial - start_trial)/60,1)\n",
    "\n",
    "        if score < 900:                                                 # after 'for loop' finishes or done, check if score is <900 then print fail         # TODO score >900\n",
    "            print(\"Finished trial {} in {} Minutes, but only reached {} points ({} tiles)\".format(trial, time_trial, round(score,0), tiles))\n",
    "            env.stats_recorder.save_complete()\n",
    "            env.stats_recorder.done = True\n",
    "\n",
    "        else:                                                           # after 'for loop' finishes or done, check if step >900 then print success\n",
    "            print(\"COMPLETED!!! reached {} points at step {} in trial {} after {} Minutes\".format(score, step, trial, time_trial))\n",
    "            agent.save_model(\"DQN/Models/{}/DQNmodel_SUCCESSFUL\".format(TRIAL_ID))\n",
    "            env.stats_recorder.save_complete()\n",
    "            env.stats_recorder.done = True\n",
    "            break\n",
    "\n",
    "        agent.save_model(\"DQN/Models/{}/DQNmodel\".format(TRIAL_ID))\n",
    "\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "TRIAL_ID = \"20201201\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "end = time.time()\n",
    "print(\"Elapsed time:\", round((end-start)/60,1),\" Minutes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Some results from a training session of the fully connected model. This training took approximately 24h with a basic machine.\n",
    "\n",
    "![Performance](Report_Resources/DQN_Full_Performance.png \"Performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video(\"Report_Resources/openaigym.video.0.1031261.video000000.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Video(\"Report_Resources/openaigym.video.0.1031261.video000018.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "There is clearly some improvement from the first episode to the 18th.\n",
    "However, Aldape and Sowell (2018) did more than 1000 epochs, to see improvements.\n",
    "In my opinion, our computational power is clearly insufficient to reach good results.\n",
    "\n",
    "### References\n",
    "\n",
    "* Aldape, Pablo, and Samuell Sowell, 2018, Reinforcement Learning for a Simple Racing Game, https://web.stanford.edu/class/aa228/reports/2018/final150.pdf."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (learny-mc-learnface)",
   "language": "python",
   "name": "pycharm-5204e121"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}